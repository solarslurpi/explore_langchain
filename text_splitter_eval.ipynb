{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal of this notebook is to gain an intuitive feel for a few of Langchain's text splitters.\n",
    "Specifically...\n",
    "- Character Splitters\n",
    "    - `CharacterTextSplitter(TextSplitter)` [RTD](https://python.langchain.com/docs/modules/data_connection/chunked_chunked_chunked_document_transformers/text_splitters/character_text_splitter)  \n",
    " This is the simplest method. This splits based on characters (by default \"\\n\\n\") and measure chunk length by number of characters.\n",
    "    - `RecursiveCharacterTextSplitter(TextSplitter)` [RTD](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)\n",
    "It starts by splitting at double newlines, then single newlines, then spaces, and finally, if necessary, character by character. This hierarchical approach ensures manageable chunks while retaining as much original structure as possible.\n",
    "- Sentence Splitters\n",
    "    - `NLTKTextSplitter(TextSplitter)` - uses NLTK's `sent_tokenizer` to split text into sentences.\n",
    "    - SpacyTextSplitter(TextSplitter) - uses it's libraries to also split text into sentences.  Before you use it, after `pip install spacy`, you need to `python -m spacy download en_core_web_sm`.\n",
    "\n",
    "All of these classes inherit from `TextSplitter()` and have default settings for chunk and overlap size [src](https://github.com/hwchase17/langchain/blob/dd648183fae95f5f251926e3a188d9ef9e6faeed/langchain/text_splitter.py#L38):\n",
    "```\n",
    "class TextSplitter(BaseDocumentTransformer, ABC):\n",
    "    \"\"\"Interface for splitting text into chunks.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 4000,\n",
    "        chunk_overlap: int = 200,\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run useful_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Splitters\n",
    "We'll start by exploring `CharacterTextSplitter` and `RecursiveCharacterTextSplitter` examples.\n",
    "\n",
    "This table provides a summary of six different configurations of text splitters: `CharacterTextSplitter` and `RecursiveCharacterTextSplitter` that are used in our exploration. Each configuration is represented by a row, detailing the text splitter's name, its abbreviated label, the chunk size (maximum characters per chunk), and the overlap size (shared characters between chunks). The chunk sizes vary from 100 to 4000 characters, and the overlap sizes are either 20 or 200 characters.The (perhaps naive) thought is these samples would be enough to gain an intuitive feel for how the text splitters work and ultimately the quality of the queries when each one is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def display_splitters_table():\n",
    "    # Define the data for the table\n",
    "    data = [\n",
    "        {'Text Splitter Name': 'CharacterTextSplitter', 'Label': 'C', 'Chunk Size': 100, 'Overlap Size': 20},\n",
    "        {'Text Splitter Name': 'CharacterTextSplitter', 'Label': 'C', 'Chunk Size': 1000, 'Overlap Size': 200},\n",
    "        {'Text Splitter Name': 'CharacterTextSplitter', 'Label': 'C', 'Chunk Size': 4000, 'Overlap Size': 200},\n",
    "        {'Text Splitter Name': 'RecursiveCharacterTextSplitter', 'Label': 'R', 'Chunk Size': 100, 'Overlap Size': 20},\n",
    "        {'Text Splitter Name': 'RecursiveCharacterTextSplitter', 'Label': 'R', 'Chunk Size': 1000, 'Overlap Size': 200},\n",
    "        {'Text Splitter Name': 'RecursiveCharacterTextSplitter', 'Label': 'R', 'Chunk Size': 4000, 'Overlap Size': 200}\n",
    "    ]\n",
    "\n",
    "    # Create the markdown table\n",
    "    table = \"| Text Splitter Name | Label | Chunk Size | Overlap Size |\\n| --- | --- | --- | --- |\\n\"\n",
    "    for row in data:\n",
    "        table += f\"| {row['Text Splitter Name']} | {row['Label']} | {row['Chunk Size']} | {row['Overlap Size']} |\\n\"\n",
    "\n",
    "    # Display the table\n",
    "    display(Markdown(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_splitters_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Our Test Text\n",
    "We'll be using a transcript from a podcast that we have on hand.  The transcript has been pickled.  Let's load the text in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import pickle\n",
    "with open('transcript.pkl', 'rb') as f:\n",
    "    transcript = pickle.load(f)\n",
    "info_str = f\"Start of the transcript:\\n\\n{transcript.page_content[:200]}...\\n\\nLength: {len(transcript.page_content)} characters.\\n\\nMetadata: {transcript.metadata}\"\n",
    "# print(info_str)\n",
    "# display(Markdown(info_str.replace(\"#\", \"\\#\")))\n",
    "display(Markdown(info_str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the text\n",
    "Next up, let's check out the code that splits the transcript text by character.  We'll be creating six chunked transcripts.  One of each using the parameters for chunk and overlap size in the previous table.  We start by creating the `split_char()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The split_char() function\n",
    "The `split_char()` function creates either a `CharacterTextSplitter` or a `RecursiveCharacterTextSplitter` depending on whether the `recursive` bool is `True` or `False`.  The intent of the function is to make it easy to create a few splitters with different chunk and overlap sizes.\n",
    "\n",
    "_Note: The separator used for `CharacterTextSplitter()` is a space.  This is because the text we'll be using does not contain either \"\\n\" or \"\\n\\n\"._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_char(chunk_size=2000, chunk_overlap=200,recursive=False):\n",
    "    if not recursive:\n",
    "        return CharacterTextSplitter(\n",
    "            # separator=\"\\n\\n\", Cannot use this. This text pattern is not in the transcripts.\n",
    "            separator = \" \",\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "    else:\n",
    "        return RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap, \n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Text into Chunks\n",
    "Now that we have `split_char`, let's chunk that text!  We'll split the text to match the chunking parameters shown earlier in the table.\n",
    "\n",
    "### Initialize a List to hold the chunks\n",
    "After chunking, we'll be able to get a better feel for how the character text splitters work.  In the future, we'll look at the quality of their QA.\n",
    "\n",
    "#### Store Chunked Transcripts in chunks_list[]\n",
    "Each entry in `chunks_list` is a chunked transcript represented as a dictionary. \n",
    " ```\n",
    " {\n",
    "    'type'\n",
    "    'chunk_size'\n",
    "    'overlap_size'\n",
    "    'chunks'\n",
    " }\n",
    " \n",
    " ```\n",
    "\n",
    "Each dictionary includes the splitting method 'type' (e.g.: 'CharacterTextSplitter'), the 'chunk_size' used for splitting, the 'overlap_size' between consecutive chunks, and the resulting 'chunks' themselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store 3 CharcterTextSplitter chunk_lists entries in chunks_list\n",
    "\n",
    "First we create a function that makes it easy for us to create a bunch of either `CharacterTextSplitter`s or `RecursiveCharacterTextSplitter`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def make_chunks(doc = None, chunk_sizes = [100, 1000, 4000], overlaps = [20, 200, 200],recursive = False):\n",
    "    # There will be a problem if len chunk_sizes != len overlaps\n",
    "\n",
    "    assert len(chunk_sizes) == len(overlaps), \"The length of the list of chunks sizes must match the length of the list of overlaps\"\n",
    "    assert doc is not None, \"The 'doc' parameter cannot be None. Please provide a valid Langchain document.\"\n",
    "    assert isinstance(doc, Document), \"The 'doc' parameter must be an instance of a Langchain Document.\"\n",
    "    chunks_list = []\n",
    "    for size, overlap in zip(chunk_sizes, overlaps):\n",
    "        c_splitter = split_char(size, overlap, recursive=recursive)\n",
    "        chunks = c_splitter.split_text(transcript.page_content)\n",
    "        type_str = \"CharacterTextSplitter\" if not recursive else \"RecursiveCharacterTextSplitter\"\n",
    "        print(f\"type: {type_str}, chunk size {size}, overlap {overlap} Num chunks: {len(chunks)}\")\n",
    "        chunks_list.append({\n",
    "            'type': type_str,\n",
    "            'chunk_size': size,\n",
    "            'overlap_size': overlap,\n",
    "            'chunks': chunks\n",
    "        })\n",
    "    return chunks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create three `CharacterTextSplitter` chunked transcript dictionaries that have the chunk and overlap size as defined in the earlier table.  Each chunked transcript dictionary becomes an entry in the `chunks_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_list = make_chunks(transcript)\n",
    "len(chunks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same for the RecursiveCharacterTextSplitter\n",
    "The only difference is to set the recursive parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_list.extend(make_chunks(transcript, recursive=True))\n",
    "len(chunks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have six samples of chunked transcripts. Each sample is distinguished by its type of splitter, chunk size, overlap size, and the corresponding chunks of text. Interestingly, we find that both the RecursiveCharacterTextSplitter and the CharacterTextSplitter have generated identical chunks. This is due to the structure of the input text, which lacks the \"\\n\\n\" or \"\\n\" markers typically present in web pages or Markdown files. These markers often indicate the start of new sections or paragraphs, and their absence in our text leads both splitters to behave similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Type | Chunk Size | Overlap | Num Chunks |\n",
       "| --- | --- | --- | --- |\n",
       "| CharacterTextSplitter | 100 | 20 | 751 |\n",
       "| CharacterTextSplitter | 1000 | 200 | 75 |\n",
       "| CharacterTextSplitter | 4000 | 200 | 16 |\n",
       "| RecursiveCharacterTextSplitter | 100 | 20 | 751 |\n",
       "| RecursiveCharacterTextSplitter | 1000 | 200 | 75 |\n",
       "| RecursiveCharacterTextSplitter | 4000 | 200 | 16 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_transcript_chunks(chunks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below, `display_splitter_results()`, provides us with some text splitting visualization.  Let's run the function to load it in memory.  Then we will be able to display characteristics of the text splitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "# Needed for running jupyter notebooks within another notebook.\n",
    "!pip install nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Character Splitting Results\n",
    "\n",
    "This table looks at text chunks from `CharacterTextSplitter` ('C') and `RecursiveCharacterTextSplitter` ('R'). Each four-row set represents two consecutive chunks, with varying chunk and overlap sizes.\n",
    "\n",
    "- **Label**: Splitter type.\n",
    "- **Chunk Size**: Maximum characters per chunk.\n",
    "- **Overlap Size**: Shared characters between chunks.\n",
    "- **Total Chunks**: Total chunks produced.\n",
    "- **Chunk #**: Specific chunk number.\n",
    "- **Position in Chunk**: 'Tail' (end) or 'Head' (start) of a chunk.\n",
    "- **Text**: Snippet from the chunk's tail or next chunk's head.\n",
    "\n",
    "The text allows us to evaluate how the overlap works between chunks.\n",
    "\n",
    "For instance, the first two rows show overlap between chunks. The 'Tail' of the first chunk and the 'Head' of the second share the phrase \"ey of KISS Organics.\", demonstrating the overlap size's role in ensuring continuity and information preservation between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_splitter_results(chunks_list,['CharacterTextSplitter','RecursiveCharacterTextSplitter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `CharacterTextSplitter` and `RecursiveCharacterTextSplitter` yield identical chunks with documents transcribed from YouTube audio via `Whisper()`, given they lack newline characters commonly found in web pages and Markdown files. We could pick another Langchain document with text that has the newline character markers.  We'll leave this for another day and more on to sentence text splitters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Splitters\n",
    "Sentence Splitters can often yield more meaningful chunks. They help maintain the semantic integrity of sentences, thus having a better chance of preserving the original text's intent after chunking. Feeding an LLM chunks of text split by a `CharacterTextSplitter` with a defined overlap size may risk breaking the continuity of sentences, leading to potential context loss. This is not a hard and fast rule, so there will be scenarios where `CharacterTextSplitter` will be a better choice.\n",
    "\n",
    "Recall from the overview, the two sentence splitters we'll explore are:\n",
    "\n",
    "    - `NLTKTextSplitter(TextSplitter)` - uses NLTK's `sent_tokenizer` to split text into sentences.\n",
    "    - SpacyTextSplitter(TextSplitter) - uses it's libraries to also split text into sentences.  Before you use it, after `pip install spacy`, you need to `python -m spacy download en_core_web_sm`.\n",
    "\n",
    "All of these classes inherit from `TextSplitter()` and have default settings for chunk and overlap size [src](https://github.com/hwchase17/langchain/blob/dd648183fae95f5f251926e3a188d9ef9e6faeed/langchain/text_splitter.py#L38):\n",
    "```\n",
    "class TextSplitter(BaseDocumentTransformer, ABC):\n",
    "    \"\"\"Interface for splitting text into chunks.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 4000,\n",
    "        chunk_overlap: int = 200,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to split text using either the `NTLKTextsplitter` or the `SpacyTextSplitter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install spacy\n",
    "!pip install nltk\n",
    "%run python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTKTextSplitter\n",
      "SpacyTextSplitter\n"
     ]
    }
   ],
   "source": [
    "nltk_splitter = NLTKTextSplitter(separator='  ')\n",
    "spacy_splitter = SpacyTextSplitter(separator='  ')\n",
    "chunks_list = [get_split_sentences(transcript, splitter_instance=nltk_splitter), get_split_sentences(transcript, splitter_instance=spacy_splitter)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Label | Chunk Size | Overlap Size | Total Chunks | Chunk # | Position in Chunk | Text |\n",
       "| --- | --- | --- | --- | --- | --- | --- |\n",
       "| N | 3894 | 0 | 16 | 1 | Tail | For our products, having the lab on site is awesome because people always ask, can you send me a certificate of analysis for your finished blends? I'm like, can do, because that's what everybody thinks that they need, which is super true. You definitely do. |\n",
       "| N | 3894 | 0 | 16 | 2 | Head | I'm like, can do, because that's what everybody thinks that they need, which is super true. You definitely do. More important than that, we do a lot of QAQC at the step before that. |\n",
       "| N | 3894 | 0 | 16 | 2 | Tail | How does tissue testing play into this, and why did you guys go that route? Well, as a company, well, the whole company started as this lab. The lab was able to give us connections to growers. |\n",
       "| N | 3894 | 0 | 16 | 3 | Head | How does tissue testing play into this, and why did you guys go that route? Well, as a company, well, the whole company started as this lab. The lab was able to give us connections to growers. |\n",
       "| S | 3701 | 0 | 17 | 1 | Tail | For our products, having the lab on site is awesome because people always ask, can you send me a certificate of analysis for your finished blends? I'm like, can do, because that's what everybody thinks that they need, which is super true. You definitely do. |\n",
       "| S | 3701 | 0 | 17 | 2 | Head | I'm like, can do, because that's what everybody thinks that they need, which is super true. You definitely do. More important than that, we do a lot of QAQC at the step before that. |\n",
       "| S | 3701 | 0 | 17 | 2 | Tail | How does tissue testing play into this, and why did you guys go that route? Well, as a company, well, the whole company started as this lab. The lab was able to give us connections to growers. |\n",
       "| S | 3701 | 0 | 17 | 3 | Head | How does tissue testing play into this, and why did you guys go that route? Well, as a company, well, the whole company started as this lab. The lab was able to give us connections to growers. |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_splitter_results(chunks_list,'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can tell from the table:\n",
    "- The chunk sizes for NLTKTextSplitter (3894 is the average chunk size) was a tad bigger than SpacyTextSplitter (3701 is the average chunk size).\n",
    "- Where CharacterTextSplitter and RecursiveCharacterTextSplitter split on characters, NLTKTextSplitter and SpacyTextSplitter split on sentences.\n",
    "- Looking at three sentences at the tail of a previous chunk with the five sentences at the head of the current chunk, it appears there is a chunk overlap of two sentences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "We could spend a lot more time on text splitters.  However, we've absorbed enough for this round and will come back another time to explore further.  At that point, we should know more about the characteristics of the Langchain classes.  On to the embedding/vector store class.\n",
    "\n",
    "Since the Character and RecursiveCharacter text splitting was pretty much identical and the NTLKTextSplitter was similar to the SpacyTextSplitter, moving forward we'll create vector databases for:\n",
    "- the three different character text splitters.\n",
    "- the `NLTKTextSplitter`.\n",
    "\n",
    "We will persist vector stores for all 4 chunked transcript and then use QAEvaluation.  The fun doesn't seem to want to stop!\n",
    "\n",
    "To finish off this step we will persist the 4 in a chunks_list pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run useful_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: CharacterTextSplitter, chunk size 100, overlap 20 Num chunks: 751\n",
      "type: CharacterTextSplitter, chunk size 1000, overlap 200 Num chunks: 75\n",
      "type: CharacterTextSplitter, chunk size 4000, overlap 200 Num chunks: 16\n",
      " There are 3 chunked transcripts.\n",
      "NLTKTextSplitter\n",
      " There are 4 chunked transcripts.\n"
     ]
    }
   ],
   "source": [
    "# Put the chunked transcript chunked with the CharacterTextSplitter into the chunks_list.\n",
    "chunks_list = make_chunks(transcript)\n",
    "print(f\" There are {len(chunks_list)} chunked transcripts.\")\n",
    "# Append the list to include the transcript chunked with the NLTKTextSplitter.\n",
    "nltk_splitter = NLTKTextSplitter(separator='  ')\n",
    "nltk_chunk_transcript = get_split_sentences(transcript, splitter_instance=nltk_splitter)\n",
    "chunks_list.append(nltk_chunk_transcript)\n",
    "print(f\" There are {len(chunks_list)} chunked transcripts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pickle the chunks_list so we can use it to create embeddings/vector stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('chunks_list.pkl', 'wb') as f:\n",
    "    pickle.dump(chunks_list, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
