{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def display_splitter_results(chunks_list, splitter_names, sentences=3):\n",
    "    assert chunks_list is not None, \"The 'chunks_list' parameter cannot be None. Please provide a valid chunk_list.\"\n",
    "    assert isinstance(chunks_list, list), \"The 'chunks_list' parameter must be a list of chunked Langchain Documents\"\n",
    "    # Abbreviate type names\n",
    "    abbreviations = {\n",
    "        'CharacterTextSplitter': 'C',\n",
    "        'RecursiveCharacterTextSplitter': 'R',\n",
    "        'NLTKTextSplitter': 'N',\n",
    "        'SpacyTextSplitter': 'S'\n",
    "    }\n",
    "    if splitter_names == 'all'.lower():\n",
    "        filtered_chunks_list = chunks_list\n",
    "    else:\n",
    "        assert all(name in abbreviations for name in splitter_names), \"Check the splitter names.  A splitter name should be either `CharacterTextSpitter`, `RecursiveTextSplitter`, `NLTKTextSplitter`, or `SpacyTextSplitter`.\"\n",
    "        filtered_chunks_list = [chunked_doc for chunked_doc in chunks_list if chunked_doc['type'] in splitter_names]\n",
    "\n",
    "    # Create a markdown string for the table\n",
    "    table = \"| Label | Chunk Size | Overlap Size | Total Chunks | Chunk # | Position in Chunk | Text |\\n| --- | --- | --- | --- | --- | --- | --- |\\n\"\n",
    "\n",
    "    for chunked_doc in filtered_chunks_list:\n",
    "        # Get the type of the splitter\n",
    "        splitter_type = abbreviations[chunked_doc['type']]\n",
    "        \n",
    "        # Get the chunk size\n",
    "        chunk_size = chunked_doc['chunk_size']\n",
    "        \n",
    "        # Get the overlap size\n",
    "        overlap_size = chunked_doc['overlap_size']\n",
    "        \n",
    "        # Get the total number of chunks\n",
    "        total_chunks = len(chunked_doc['chunks'])\n",
    "        \n",
    "        # Get the contents for the first two chunks\n",
    "        for i in range(min(2, total_chunks - 1)):\n",
    "            chunk = chunked_doc['chunks'][i]\n",
    "            next_chunk = chunked_doc['chunks'][i+1]\n",
    "            \n",
    "            if chunked_doc['type'] in ['NLTKTextSplitter', 'SpacyTextSplitter']:\n",
    "                # Tokenize the tail and head into sentences\n",
    "                tail_sentences_list = sent_tokenize(chunk[-overlap_size:])\n",
    "                head_sentences_list = sent_tokenize(' '.join(sent_tokenize(next_chunk)))\n",
    "                \n",
    "                # Get the last few sentences of the tail and the first few sentences of the head\n",
    "                tail = ' '.join(tail_sentences_list[-sentences:]) if tail_sentences_list else ''\n",
    "                head = ' '.join(head_sentences_list[:sentences]) if head_sentences_list else ''\n",
    "            else:\n",
    "                # Don't tokenize the tail and head\n",
    "                tail = chunk[-overlap_size:]\n",
    "                head = next_chunk[:overlap_size+50]\n",
    "            \n",
    "            # Add a row to the table for the tail of the current chunk\n",
    "            table += f\"| {splitter_type} | {chunk_size} | {overlap_size} | {total_chunks} | {i+1} | Tail | {tail} |\\n\"\n",
    "            \n",
    "            # Add a row to the table for the head of the next chunk\n",
    "            table += f\"| {splitter_type} | {chunk_size} | {overlap_size} | {total_chunks} | {i+2} | Head | {head} |\\n\"\n",
    "\n",
    "    # Display the table\n",
    "    display(Markdown(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "###  Transcript chunks\n",
    "def display_transcript_chunks(chunks_list):\n",
    "    assert chunks_list is not None, \"The 'chunks_list' parameter cannot be None. Please provide a valid chunk_list.\"\n",
    "    assert isinstance(chunks_list, list), \"The 'chunks_list' parameter must be a list of chunked Langchain Documents\"\n",
    "    # Create the header of the table\n",
    "    table = \"| Type | Chunk Size | Overlap | Num Chunks |\\n| --- | --- | --- | --- |\\n\"\n",
    "\n",
    "    # Add each row to the table\n",
    "    for transcript_split_dict in chunks_list:\n",
    "        table += f\"| {transcript_split_dict['type']} | {transcript_split_dict['chunk_size']} | {transcript_split_dict['overlap_size']} | {len(transcript_split_dict['chunks'])} |\\n\"\n",
    "\n",
    "    # Display the table\n",
    "    display(Markdown(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import NLTKTextSplitter, SpacyTextSplitter \n",
    "def get_split_sentences(doc, splitter_instance):\n",
    "    assert doc is not None, \"The 'doc' parameter cannot be None. Please provide a valid Langchain document.\"\n",
    "    assert isinstance(doc, Document), \"The 'doc' parameter must be an instance of a Langchain Document.\"\n",
    "    assert isinstance(splitter_instance, (NLTKTextSplitter, SpacyTextSplitter)), \"The 'splitter_instance' parameter must be an instance of either NLTKTextSplitter or SpacyTextSplitter.\"\n",
    "    class_name = splitter_instance.__class__.__name__\n",
    "    print(class_name)\n",
    "    chunks = splitter_instance.split_text(doc.page_content)\n",
    "    chunk_sizes = [len(chunk) for chunk in chunks]\n",
    "    average_chunk_size = round(sum(chunk_sizes) / len(chunk_sizes))\n",
    "    # chunks = splitter_instance(separator='  ').split_text(doc.page_content)\n",
    "    # I set the chunk and overlap sizes to 0 because the splitter figures out.\n",
    "    return {\n",
    "        'type': splitter_instance.__class__.__name__,\n",
    "        'chunk_size': average_chunk_size,\n",
    "        'overlap_size': 0,\n",
    "        'num_chunks' : len(chunks),\n",
    "        'chunks': chunks\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken.model import MODEL_TO_ENCODING\n",
    "\n",
    "\n",
    "# Now MODEL_TO_ENCODING is a dictionary where keys are model names and values are their encodings\n",
    "# We can transform it into a list of dictionaries\n",
    "def utils_get_llm_names_and_encoding() -> list:\n",
    "    list_of_dicts = [{\"model\": k, \"encoding\": v} for k, v in MODEL_TO_ENCODING.items()]\n",
    "    return list_of_dicts    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
